{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "join = os.path.join\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import monai\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loads dataset based on the filename (should be a .npz file)\n",
    "\"\"\"\n",
    "\n",
    "class NpzDataset(Dataset): \n",
    "    def __init__(self, npz_filename):\n",
    "        self.npz_data = np.load(npz_filename)\n",
    "        self.ori_gts = self.npz_data['gts']\n",
    "        self.img_embeddings = self.npz_data['img_embeddings']\n",
    "        print(f\"{self.img_embeddings.shape=}, {self.ori_gts.shape=}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ori_gts.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_embed = self.img_embeddings[index]\n",
    "        gt2D = self.ori_gts[index]\n",
    "        y_indices, x_indices = np.where(gt2D > 0)\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "        # add perturbation to bounding box coordinates\n",
    "        H, W = gt2D.shape\n",
    "        x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "        x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "        y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "        y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "        bboxes = np.array([x_min, y_min, x_max, y_max])\n",
    "        # convert img embedding, mask, bounding box to torch tensor\n",
    "        return torch.tensor(img_embed).float(), torch.tensor(gt2D[None, :,:]).long(), torch.tensor(bboxes).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 10\n",
    "BETA = 0.75\n",
    "GAMMA = 10.\n",
    "\n",
    "class GradReverse(Function):\n",
    "    lambd = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * GradReverse.lambd    \n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper for the run of encoder-rg-discriminator in order to run net in one\n",
    "    back-propagation as described in paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, discriminator):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.discriminator = discriminator\n",
    "        self.lambd = 0\n",
    "\n",
    "    def update_lambd(self, lambd):\n",
    "        self.lambd = lambd\n",
    "        GradReverse.lambd = self.lambd\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.encoder(input)\n",
    "        x = GradReverse.apply(x)\n",
    "        x = self.discriminator(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A classifier architecture for mnist data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 128, kernel_size=2, stride=2)\n",
    "        self.dec_conv1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 256, kernel_size=8, stride=8) # Upsample to 256x256\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.encode(input)\n",
    "        # x = torch.flatten(x, start_dim=1)\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "\n",
    "    def encode(self, input):\n",
    "        x = F.relu(self.conv1(input))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "    def decode(self, input):\n",
    "        x = F.relu(self.upconv1(input))\n",
    "        x = F.relu(self.dec_conv1(x))\n",
    "        x = self.upconv2(x) # No activation, assuming a multi-class segmentation task\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    A discriminator architecture adapted for input feature maps with 64 channels.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Convolutional Encoder\n",
    "        self.conv1 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)  # Adjusted to 64 input channels\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=4, stride=2, padding=1)   # Output: 64 x 8 x 8\n",
    "        self.conv3 = nn.Conv2d(64, 32, kernel_size=4, stride=2, padding=1)    # Output: 32 x 4 x 4\n",
    "\n",
    "        # Flatten and Dense Layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(128, 100)  # Adjusted to match the new flattened conv output\n",
    "        self.dense2 = nn.Linear(100, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Convolutional layers\n",
    "        x = F.leaky_relu(self.conv1(input))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "\n",
    "        # Flatten and dense layers\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.dense2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRDomainAdaptation:\n",
    "\n",
    "    def __init__(self):\n",
    "        ###########################\n",
    "        # Initialize Info Holders #\n",
    "        ########################### \n",
    "        # self.args = get_params(source_dataset, experiment='adaptation')\n",
    "        self.source_dataset = 'mnist'\n",
    "        self.target_dataset = 'mnist_m'\n",
    "        self.n_epochs = 50\n",
    "        self.batch_size_train = 64\n",
    "        self.batch_size_test = 1000\n",
    "        self.learning_rate = 0.01\n",
    "        self.momentum = 0.9\n",
    "        self.log_interval = 10\n",
    "\n",
    "        self.random_seed = 1\n",
    "        self.cuda = True\n",
    "        self.check_pth = '/home/ubuntu/nadav/GradientReversal/weights/mnist2mnist_m'\n",
    "\n",
    "\n",
    "        self.source_best_pred = 0.0\n",
    "        self.target_best_pred = 0.0\n",
    "        self.best_source_net_state = None\n",
    "        self.best_target_net_state = None\n",
    "        self.source_test_losses = []\n",
    "        self.target_test_losses = []\n",
    "        self.source_test_acc = []\n",
    "        self.target_test_acc = []\n",
    "        self.iters = 0\n",
    "\n",
    "        #######################################\n",
    "        # Initialize Source and target labels #\n",
    "        #######################################\n",
    "        self.source_disc_labels = torch.zeros(size=(self.batch_size_train, 1)).requires_grad_(False)\n",
    "        self.target_disc_labels = torch.ones(size=(self.batch_size_train, 1)).requires_grad_(False)\n",
    "        if self.cuda:\n",
    "            self.source_disc_labels = self.source_disc_labels.cuda()\n",
    "            self.target_disc_labels = self.target_disc_labels.cuda()\n",
    "\n",
    "        \"\"\"\n",
    "        Load the source and target dataset into a dataloader for training\n",
    "        \"\"\"\n",
    "        self.source_dataset = NpzDataset('data/demo2D_vit_b/source_dataset.npz')\n",
    "        self.target_dataset = NpzDataset('data/demo2D_vit_b/target_dataset.npz')\n",
    "\n",
    "        self.source_train_loader = DataLoader(self.source_dataset, batch_size=8, shuffle=True)\n",
    "        self.target_train_loader = DataLoader(self.target_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "        self.n_batch = min(len(self.target_train_loader), len(self.source_train_loader))\n",
    "\n",
    "        ##################\n",
    "        # Define network #\n",
    "        ##################\n",
    "        self.net = Classifier()\n",
    "\n",
    "        if self.cuda:\n",
    "            self.net = torch.nn.DataParallel(self.net, device_ids=[0])\n",
    "            self.net = self.net.cuda()\n",
    "\n",
    "        ###############\n",
    "        # Set Encoder #\n",
    "        ###############\n",
    "        if self.cuda:\n",
    "            self.encoder = self.net.module.encode\n",
    "        else:\n",
    "            self.encoder = self.net.encode\n",
    "\n",
    "        device = 'cuda:0'\n",
    "        checkpoint = 'work_dir/SAM/sam_vit_b_01ec64.pth'\n",
    "        model_type = 'vit_b'        \n",
    "        self.net = sam_model_registry[model_type](checkpoint=checkpoint).to(device)\n",
    "\n",
    "\n",
    "        ###################################################\n",
    "        # Set Domain Classifier (Encoder + Discriminator) #\n",
    "        ###################################################\n",
    "        self.discriminator = Discriminator()\n",
    "        self.domain_classifier = DomainClassifier(self.encoder, self.discriminator)\n",
    "        if self.cuda:\n",
    "            self.domain_classifier = torch.nn.DataParallel(self.domain_classifier, device_ids=[0])\n",
    "            self.domain_classifier = self.domain_classifier.cuda()\n",
    "\n",
    "        #####################\n",
    "        # Define Optimizers #\n",
    "        #####################\n",
    "        self.net_optimizer = torch.optim.Adam(self.net.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "        self.encoder_optimizer = torch.optim.SGD(self.net.parameters(), self.learning_rate, momentum=self.momentum)\n",
    "        self.discriminator_optimizer = torch.optim.SGD(self.discriminator.parameters(), lr=self.learning_rate, momentum=self.momentum)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \n",
    "        self.net.train()\n",
    "        tbar = tqdm(enumerate(zip(self.source_train_loader, self.target_train_loader)))\n",
    "        net_loss = 0.0\n",
    "        disc_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i, ((image_embedding, gt2D, boxes), (target_img, _, _)) in enumerate((zip(self.source_train_loader, self.target_train_loader))):\n",
    "            ##############################\n",
    "            # update learning parameters #\n",
    "            ##############################\n",
    "            print(i)\n",
    "            self.iters += 1\n",
    "            p = self.iters / (self.n_epochs * self.n_batch)\n",
    "\n",
    "            lambd = (2. / (1. + np.exp(-GAMMA * p))) - 1\n",
    "            if self.cuda:\n",
    "                self.domain_classifier.module.update_lambd(lambd)\n",
    "            else:\n",
    "                self.domain_classifier.update_lambd(lambd)\n",
    "\n",
    "            lr = self.learning_rate / (1. + ALPHA * p) ** BETA\n",
    "            self.discriminator_optimizer.lr = lr\n",
    "            self.net_optimizer.lr = lr\n",
    "            self.encoder_optimizer.lr = lr\n",
    "\n",
    "            #########################################################################\n",
    "            # set batch size in cases where source and target domain differ in size #\n",
    "            #########################################################################\n",
    "            curr_batch_size = min(image_embedding.shape[0], target_img.shape[0])\n",
    "            # image_embedding = image_embedding[:curr_batch_size]\n",
    "            # gt2D = gt2D[:curr_batch_size]\n",
    "            # target_img = target_img[:curr_batch_size]\n",
    "            source_disc_labels = self.source_disc_labels[:curr_batch_size]\n",
    "            target_disc_labels = self.target_disc_labels[:curr_batch_size]\n",
    "            if self.cuda:\n",
    "                image_embedding, gt2D = image_embedding.cuda(), gt2D.cuda()\n",
    "                target_img = target_img.cuda()\n",
    "\n",
    "            #######################################################\n",
    "            # Train network (Encoder + Classifier) on Source Data #\n",
    "            #######################################################\n",
    "            device = 'cuda:0'\n",
    "            with torch.no_grad():\n",
    "                # convert box to 1024x1024 grid\n",
    "                box_np = boxes.numpy()\n",
    "                sam_trans = ResizeLongestSide(self.net.image_encoder.img_size)\n",
    "                box = sam_trans.apply_boxes(box_np, (gt2D.shape[-2], gt2D.shape[-1]))\n",
    "                box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "                if len(box_torch.shape) == 2:\n",
    "                    box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
    "                # get prompt embeddings \n",
    "                sparse_embeddings, dense_embeddings = self.net.prompt_encoder(\n",
    "                    points=None,\n",
    "                    boxes=box_torch,\n",
    "                    masks=None,\n",
    "                )\n",
    "\n",
    "            net_output, _ = self.net.mask_decoder(\n",
    "            image_embeddings=image_embedding.to(device), # (B, 256, 64, 64)\n",
    "            image_pe=self.net.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "          )\n",
    "            seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "            class_net_loss = seg_loss(net_output, gt2D.to(device))\n",
    "            self.net_optimizer.zero_grad()\n",
    "            class_net_loss.backward()\n",
    "            self.net_optimizer.step()\n",
    "            net_loss += class_net_loss\n",
    "\n",
    "            #########################################\n",
    "            # Train encoder on Source + Target data #\n",
    "            #########################################\n",
    "            self.encoder_optimizer.zero_grad()\n",
    "            self.discriminator_optimizer.zero_grad()\n",
    "            dom_input = torch.cat([image_embedding, target_img], dim=0)\n",
    "            dom_labels = torch.cat([source_disc_labels, target_disc_labels], dim=0)\n",
    "            dom_output = self.domain_classifier(dom_input)\n",
    "            dom_loss = F.binary_cross_entropy(dom_output, dom_labels)\n",
    "\n",
    "            # calculate total loss value\n",
    "            dom_loss.backward()\n",
    "            self.discriminator_optimizer.step()\n",
    "            self.encoder_optimizer.step()\n",
    "            disc_loss += dom_loss\n",
    "\n",
    "            total_loss += class_net_loss - lambd * dom_loss\n",
    "            tbar.set_description('Net loss: {0:.6f}; Discriminator loss: {1:.6f}; Total Loss: {2:.6f}; {3:.2f}%;'.format((net_loss / (i + 1)),\n",
    "                                                                                                                         (disc_loss / (i + 1)),\n",
    "                                                                                                                         (total_loss / (i + 1)),\n",
    "                                                                                                                        (i + 1) / self.n_batch * 100))\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            print('Epoch: {}; Source Best: {}; Target Best: {}'.format(epoch, self.source_best_pred, self.target_best_pred))\n",
    "            self.train_epoch()\n",
    "        output_dir = 'work_dir/demo2D/'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # torch.save(self.best_source_net_state, os.path.join(output_dir, 'source_model.pth'))\n",
    "        torch.save(self.best_target_net_state, os.path.join(output_dir, 'sam_model_best.pth'))\n",
    "\n",
    "trainer = GRDomainAdaptation()\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
