{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "join = os.path.join\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import monai\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_from_mask(mask):\n",
    "    '''Returns a bounding box from a mask'''\n",
    "    # Check if the mask contains any non-zero values\n",
    "    if np.count_nonzero(mask) == 0:\n",
    "        # If the mask is entirely black, return a default bounding box\n",
    "        return np.array([0, 0, 1, 1])  # Default bounding box with width and height of 1\n",
    "    \n",
    "    # Get indices of non-zero elements\n",
    "    y_indices, x_indices = np.where(mask > 0)\n",
    "    # Compute bounding box\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    # Add perturbation to bounding box coordinates\n",
    "    H, W = mask.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "\n",
    "    # Ensure positive width and height\n",
    "    width = max(1, x_max - x_min)\n",
    "    height = max(1, y_max - y_min)\n",
    "\n",
    "    return np.array([x_min, y_min, x_min + width, y_min + height])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"labels\"))))\n",
    "        print(f'imgs len: {len(self.imgs)}, masks len: {len(self.masks)}')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load images and masks\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"labels\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = np.array(mask)\n",
    "        # Get bounding box coordinates for each mask\n",
    "        bbox = get_bbox_from_mask(mask)\n",
    "        # Convert everything into a torch.Tensor\n",
    "        bbox = torch.as_tensor([bbox], dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)  # assuming only one class\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = bbox\n",
    "        target[\"labels\"] = labels\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "# Transformations\n",
    "def get_transform():\n",
    "    def transform(img):\n",
    "        return F.to_tensor(img)\n",
    "    return transform\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CustomDataset(root=\"data/MedSAMDemo_2D/train\", transforms=get_transform())\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "for i, (img, target) in enumerate(data_loader):\n",
    "    if i==0:\n",
    "        print(img[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    # load a model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "\n",
    "    return model\n",
    "\n",
    "# Assuming only one class (apart from background), num_classes = 2 (class + background)\n",
    "model = get_model(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Move model to the right device\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 10\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "\n",
    "lowest_loss = float('inf')\n",
    "best_model_path = 'bbox_model/best_bbox_model.pth'  # Path to save the best model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, (imgs, targets) in enumerate(tqdm(data_loader)):\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(imgs, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if losses.item() < lowest_loss:\n",
    "        lowest_loss = losses.item()\n",
    "        # Save the model's parameters\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss: {losses.item()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
